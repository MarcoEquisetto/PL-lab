{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning-Lab Lesson 2: Solving Probabilistic Markov Decision Processes\n",
    "\n",
    "In this second session, we will work on solving problems modelled as goal-based Markov decision processes (also known as Stochastic Shortest Path).\n",
    "\n",
    "## The environment\n",
    "The environment used is the same one introduced during theoretical lectures, with some variations:\n",
    "\n",
    "<img src=\"images/env.png\" width=\"600\">\n",
    "\n",
    "#### The agent starts in state **d1** and has to reach state **d4**. Some transitions from one state to another have a stochastic outcome (e.g. going from d2 to d3 has a 20% chance of ending in d5) and different costs (i.e. negative rewards, the ones in purple in the figure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "# Add the tools directory to the path\n",
    "tools_path = os.path.abspath(os.path.join(os.getcwd(), '../tools'))\n",
    "if tools_path not in sys.path:\n",
    "    sys.path.insert(0, tools_path)\n",
    "\n",
    "# Add the utils directory to the path\n",
    "utils_path = os.path.join(tools_path, 'utils')\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.insert(0, utils_path)\n",
    "\n",
    "from ai_lab_functions import UCT_log\n",
    "import gym\n",
    "from envs import *\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Properties \n",
    "\n",
    "Here you have some useful functions you can use in the environment:\n",
    "- **Pr(s' | s, a)**: returns the probability of ending in state **s'** starting from **s** and performing action **a**;\n",
    "- **$\\gamma$(s, a)**: returns all the states in which the agent could end performing action **a** in **s**;\n",
    "- **Applicable(s)**: returns the actions that can be executed from state **s**;\n",
    "- **get_cost(s, a, s')**: return the cost (a negative reward) of performing action **a** to go from state **s** to state **s'**.\n",
    "\n",
    "#### Code Hints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions:  11\n",
      "Actions:  ['m12', 'm14', 'm21', 'm23', 'm32', 'm34', 'm41', 'm43', 'm45', 'm52', 'm54']\n",
      "States:  ['d1', 'd2', 'd3', 'd4', 'd5']\n",
      "Probability from d1 to d2 with action m12: 1.0\n",
      "Actions that can be performed in state d2: ['m23', 'm21']\n",
      "States in which action m23 performed in state d2 could end: ['d3', 'd5']\n",
      "Cost (already converted into a negative reward) for performing m12 from d1 to d2: -100\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"ProbabilisticPlanningDomain-v0\")\n",
    "\n",
    "print(\"Number of actions: \", env.action_space.n)\n",
    "print(\"Actions: \", env.actions)\n",
    "print(\"States: \", env.states)\n",
    "print(\"Probability from d1 to d2 with action m12:\", env.Pr[('d1', 'm12', 'd2')])\n",
    "print(\"Actions that can be performed in state d2:\", env.Applicable('d2'))\n",
    "print(\"States in which action m23 performed in state d2 could end:\", env.gamma('d2', 'm23'))\n",
    "print(\"Cost (already converted into a negative reward) for performing m12 from d1 to d2:\", env.get_cost('d1', 'm12', 'd2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Policy Iteration Algorithm\n",
    "\n",
    "Your first assignment is to implement the Policy Iteration algorithm on the environment presented above. The solution returned by your algorithm must be a 1-d array of action identifiers where the $i$-th action refers to the $i$-th state. \n",
    "\n",
    "<img src=\"images/policy-iteration.png\" width=\"600\">\n",
    "\n",
    "For the *policy evaluation step*, it is necessary to implement this function:\n",
    "\n",
    "<img src=\"images/policy-evaluation.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following function has to be implemented:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(environment, maxiters=200, maxviters=50, delta=1e-3):\n",
    "    i = 1\n",
    "    # Initialize a safe policy\n",
    "    pi = environment.init_safe_policy()\n",
    "    # Initialize Value Function\n",
    "    V = {s: 0 for s in environment.states}\n",
    "    \n",
    "    while i <= maxiters:\n",
    "        print(\"iteration\", i)\n",
    "        i += 1\n",
    "        \n",
    "        # 1) Policy Evaluation\n",
    "        for j in range(maxviters):\n",
    "            max_diff = 0\n",
    "            for state in environment.states:\n",
    "                # Value of goal state is always 0\n",
    "                if state == environment.goal_state:\n",
    "                    V[state] = 0\n",
    "                    continue\n",
    "                \n",
    "                action = pi.get(state)\n",
    "                # If no action is defined (or terminal), skip\n",
    "                if action is None:\n",
    "                    continue\n",
    "                \n",
    "                # Bellman Expectation Equation for V\n",
    "                # V(s) = sum_{s'} P(s'|s,a) * [ Cost(s,a,s') + V(s') ]\n",
    "                val = 0\n",
    "                # Get all possible next states from this state-action pair\n",
    "                possible_next_states = environment.gamma(state, action)\n",
    "                \n",
    "                for next_state in possible_next_states:\n",
    "                    prob = environment.Pr.get((state, action, next_state), 0)\n",
    "                    cost = environment.get_cost(state, action, next_state)\n",
    "                    val += prob * (cost + V[next_state])\n",
    "                \n",
    "                max_diff = max(max_diff, abs(V[state] - val))\n",
    "                V[state] = val\n",
    "            \n",
    "            # Check for convergence of Value Evaluation\n",
    "            if max_diff < delta:\n",
    "                break\n",
    "        \n",
    "        # 2) Policy Improvement\n",
    "        unchanged = True  \n",
    "        for state in environment.states:\n",
    "            if state == environment.goal_state:\n",
    "                continue\n",
    "            \n",
    "            old_action = pi.get(state)\n",
    "            \n",
    "            best_action = old_action\n",
    "            best_value = -float('inf') \n",
    "            \n",
    "            # Argmax over actions\n",
    "            applicable_actions = environment.Applicable(state)\n",
    "            \n",
    "            for action in applicable_actions:\n",
    "                q_val = 0\n",
    "                possible_next_states = environment.gamma(state, action)\n",
    "                \n",
    "                for next_state in possible_next_states:\n",
    "                    prob = environment.Pr.get((state, action, next_state), 0)\n",
    "                    cost = environment.get_cost(state, action, next_state)\n",
    "                    q_val += prob * (cost + V[next_state])\n",
    "                \n",
    "                # We strictly maximize the value (negative cost)\n",
    "                if q_val > best_value:\n",
    "                    best_value = q_val\n",
    "                    best_action = action\n",
    "            \n",
    "            if best_action != old_action:\n",
    "                pi[state] = best_action\n",
    "                unchanged = False\n",
    "            \n",
    "        if unchanged: \n",
    "            print(f\"STOP [unchanged = {unchanged}]\")\n",
    "            break       \n",
    "    \n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code executes Value Iteration and prints the resulting policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INITIAL SAFE POLICY: \n",
      "{'d1': 'm14', 'd2': 'm23', 'd3': 'm34', 'd4': None, 'd5': 'm54'}\n",
      "iteration 1\n",
      "iteration 2\n",
      "STOP [unchanged = True]\n",
      "\n",
      "EXECUTION TIME: 0.0008\n",
      "{'d1': 'm12', 'd2': 'm23', 'd3': 'm34', 'd4': None, 'd5': 'm54'}\n",
      "Goal State is: d4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"ProbabilisticPlanningDomain-v0\")\n",
    "env.reset()\n",
    "\n",
    "t = timer()\n",
    "print(\"\\nINITIAL SAFE POLICY: \\n{}\".format(env.init_safe_policy()))\n",
    "\n",
    "policy = policy_iteration(env)\n",
    "\n",
    "print(\"\\nEXECUTION TIME: {}\".format(round(timer() - t, 4)))\n",
    "print(policy)\n",
    "print(f\"Goal State is: {env.goal_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upper Confidence Trees (UCT) Algorithm\n",
    "\n",
    "Your second assignment is to implement parts of the UCT algorithm on LavaFloor. This time, the solution returned by your algorithm will be the preferred action to accomplish in a given state, not a full policy. You can perform some preliminary tests on the previous environment and then test your algorithm on the new environment explained below.\n",
    "\n",
    "## Description\n",
    "The **Upper Confidence Trees (UCT)** algorithm is a sampling-based method that balances *exploration* and *exploitation* when searching through state-action spaces.\n",
    "\n",
    "<img src=\"images/uct_lookahead.png\" width=\"400\">\n",
    "\n",
    "The main **UCT_Lookahead Function** iteratively invokes rollouts until the termination condition is met. After the process, it determines the best action based on the Q-values. \n",
    "This implementation applies UCT with a recursive rollout process to evaluate and select the best action in a given state based on accumulated Q-values and visit counts. It operates under a depth-limited search and terminates based on the number of simulations performed.\n",
    "\n",
    "<img src=\"images/rollout.png\" width=\"600\">\n",
    "\n",
    "The recursive **UCT Rollout Function** selects actions based on the exploration-exploitation tradeoff to simulate transitions to new states and uses the accumulated cost to update the Q-values for each state-action pair.\n",
    "The action selection strategy is implemented using a UCT-based formula:\n",
    "\n",
    "<img src=\"images/select-UCT-max.png\" width=\"600\">\n",
    "\n",
    "   where:\n",
    "   - Q(s, a): Action value for state-action pair (s, a);\n",
    "   - N(s): Total visit count for state s;\n",
    "   - N(s, a): Visit count for state-action pair (s, a);\n",
    "   - C: Exploration-exploitation tradeoff constant.\n",
    "\n",
    "The **MDP_Lookahead** function repeatedly calls the UCT_Lookahead to select the next action and execute the selected action (using the step() method) until a goal state is reached:\n",
    "\n",
    "<img src=\"images/mdp_lookahead.png\" width=\"300\">\n",
    "   \n",
    "Finally, we will need some **helper Functions** that depend on the environment to which we apply the UCT algorithm:\n",
    "   - `V0`: Provides a heuristic value for terminal states.\n",
    "   - `Sample`: Simulates a transition to a new state given a state-action pair.\n",
    "\n",
    "Notice the difference between the step() method and the Sample() method: the step method executes the action in the world where the agent is operating while the Sample() method simulates the execution of the action to select the next best action through the UCT Rollout process. In this exercise, we use a copy of the environments where the agent operates to simulate the execution, hence the UCT rollout will be very accurate and the process very efficient. In real-life scenarios, we typically do not have such an accurate simulator/model of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sample(sim_env, s, a):\n",
    "    sim_env.state = s\n",
    "    next_state, _, _, _ = sim_env.step(a)\n",
    "    return next_state\n",
    "def V0(s):\n",
    "    return 0\n",
    "def Select(sim_env, s, Q, N):\n",
    "    if N[s] == 0:\n",
    "        import random\n",
    "        return random.choice(sim_env.Applicable(s))\n",
    "\n",
    "    c = math.sqrt(2)\n",
    "    \n",
    "    # We use N[s] inside the log, which is now guaranteed to be > 0\n",
    "    return max(sim_env.Applicable(s), key=lambda a: Q[s].get(a, 0) + c * math.sqrt(math.log(N[s]) / (1 + N.get((s, a), 0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCT_rollout(s, h, sim_env, Envelope=None, Q=None, N=None, metrics=None):\n",
    "    \"\"\"\n",
    "    UCT-Rollout implementation.\n",
    "    \n",
    "    Parameters:\n",
    "        s: Current state\n",
    "        h: Current depth\n",
    "        sim_env: Environment \n",
    "        Envelope: Set to store visited states\n",
    "        Q: Dictionary for Q-values\n",
    "        N: Dictionary for visit counts\n",
    "        metrics: Dictionary to track evaluation metrics\n",
    "    \"\"\"\n",
    "    # Initialize Envelope, Q, and N if not provided\n",
    "    if Envelope is None:\n",
    "        Envelope = set()\n",
    "    if Q is None:\n",
    "        Q = {}\n",
    "    if N is None:\n",
    "        N = {}\n",
    "\n",
    "    # Base cases\n",
    "    if s in sim_env.S_g():\n",
    "        return 0\n",
    "    if h == 0:\n",
    "        return V0(s)\n",
    "    \n",
    "    if s not in Envelope:\n",
    "        Envelope.add(s)\n",
    "        N[s] = 0\n",
    "        Q[s] = {}\n",
    "        for a in sim_env.Applicable(s):\n",
    "            Q[s][a] = 0\n",
    "            N[(s,a)] = 0\n",
    "\n",
    "    a = Select(sim_env, s, Q, N)\n",
    "    s_next = Sample(sim_env, s, a)\n",
    "    cost_rollout = sim_env.get_cost(s, a, s_next) + UCT_rollout(s_next, h-1, sim_env, Envelope, Q, N, metrics)\n",
    "    Q[s][a] = (Q[s].get(a, 0) * N.get((s,a), 0) + cost_rollout) / (N.get((s,a), 0) + 1)\n",
    "    N[s] += 1\n",
    "    N[(s,a)] += 1\n",
    "\n",
    "    return cost_rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "def UCT_Lookahead(s, h, n_sim, env, metrics):\n",
    "    \"\"\"\n",
    "    UCT algorithm implementation.\n",
    "\n",
    "    Parameters:\n",
    "        s: Initial state\n",
    "        h: Depth limit\n",
    "        n_sim: The number of simulations to perform, this represents the termination condition\n",
    "        env: environment in which to perform the procedure\n",
    "        metrics: dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    if len(env.Applicable(s)) != 0:\n",
    "        sim_env = copy.deepcopy(env)\n",
    "        Q = {}\n",
    "        N = {}\n",
    "        Envelope = set()\n",
    "        start_time = time.time()\n",
    "        for _ in range(n_sim):\n",
    "            metrics[\"total_iterations\"] += 1\n",
    "            UCT_rollout(s, h, sim_env, Envelope, Q, N, metrics)\n",
    "        metrics[\"total_time\"] += time.time() - start_time\n",
    "        if s in Q:\n",
    "            best_action = max(Q[s], key=Q[s].get)\n",
    "            return best_action\n",
    "    else:\n",
    "        print(\"Dead end state reached\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_Lookahead(env, depth, simulations):\n",
    "\n",
    "    eval_metrics = {\n",
    "        \"total_iterations\": 0,\n",
    "        \"total_time\": 0,\n",
    "        \"new_action_count\": 0,\n",
    "        \"tried_action_count\": 0\n",
    "    }\n",
    "    \n",
    "    state = env.reset()          \n",
    "    done = False\n",
    "    while not done:\n",
    "        action = UCT_Lookahead(state, depth, simulations, env, eval_metrics)\n",
    "        print(UCT_log(env=env, state=state, action=action))\n",
    "        state, _, done, _ = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode terminated in state \", state)\n",
    "\n",
    "    return state, eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed action m14 in state d1\n",
      "Executed action m14 in state d1\n",
      "Episode terminated in state  d4\n",
      "\n",
      "Goal state reached!\n",
      "\n",
      "--- Evaluation Metrics ---\n",
      "Total Iterations: 100\n",
      "Total Time (seconds): 0.0009\n",
      "New Action Count: 0\n",
      "Tried Action Count: 0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"ProbabilisticPlanningDomain-v0\")\n",
    "state, metrics = MDP_Lookahead(env, 50, 50)\n",
    "if state == env.goal_state:    \n",
    "    print(\"\\nGoal state reached!\")\n",
    "\n",
    "print(\"\\n--- Evaluation Metrics ---\")    \n",
    "print(f\"Total Iterations: {metrics['total_iterations']}\")\n",
    "print(f\"Total Time (seconds): {metrics['total_time']:.4f}\")\n",
    "print(f\"New Action Count: {metrics['new_action_count']}\")\n",
    "print(f\"Tried Action Count: {metrics['tried_action_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lava environments\n",
    "You can test the UCT algorithm you implemented on the LavaFloor environment (visible in the figure) and its variations.\n",
    "\n",
    "![Lava](images/lava.png)\n",
    "\n",
    "The agent starts in cell $(0, 0)$ and has to reach the treasure in $(2, 3)$. In addition to the walls the agent can't cross, the floor is covered with lava, and there is a black pit of death.\n",
    "\n",
    "Moreover, the agent can't comfortably perform its actions that instead have a stochastic outcome (visible in the figure):\n",
    "\n",
    "![Dynact](images/dynact.png)\n",
    "\n",
    "The available actions are Left, Right, Up and Down, denoted in the environment as \"L\", \"R\", \"U\", \"D\".\n",
    "The action dynamics is the following:\n",
    "- $P(0.8)$ of moving **in the desired direction**\n",
    "- $P(0.1)$ of moving in a direction 90Â° with respect to the desired direction\n",
    "\n",
    "Finally, since the floor is covered in lava, the agent receives a negative reward for each of its steps!\n",
    "\n",
    "- -0.04 for each lava cell (L)\n",
    "- -5 for the black pit (P). End of episode\n",
    "- +1 for the treasure (G). End of episode\n",
    "\n",
    "\n",
    "### Environment Properties \n",
    "In addition to the variables available in the previous environment, there are also a few more:\n",
    "\n",
    "- $T$: matrix of the transition function $T(s, a, s') \\rightarrow [0, 1]$\n",
    "- $RS$: matrix of the reward function $R(s) \\rightarrow \\mathbb{R}$\n",
    "\n",
    "\n",
    "#### Code Hints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENVIRONMENT:\n",
      "\n",
      "[['S' 'L' 'L' 'L']\n",
      " ['L' 'W' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n",
      "\n",
      "Number of actions:  4\n",
      "Actions:  {0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n",
      "Reward of starting state: -0.04\n",
      "Reward of goal state: 1.0\n",
      "Probability from (0, 0) to (1, 0) with action left: 0.8\n",
      "Probability from (0, 0) to (2, 3) with action left: 0.0\n",
      "\n",
      "Percentage of time agent reaches the state to the right: 76.0\n",
      "\n",
      "Transition model for LavaFloor-v0 :\n",
      "\n",
      "Probability of reaching (0, 1) from (0, 0) executing action L : 0.0\n",
      "\n",
      "Probability of reaching (0, 1) from (0, 0) executing action R : 0.8\n",
      "\n",
      "Probability of reaching (0, 1) from (0, 0) executing action U : 0.1\n",
      "\n",
      "Probability of reaching (0, 1) from (0, 0) executing action D : 0.1\n",
      "\n",
      "Reward for non terminal states:  -0.04\n",
      "Reward for state: (1, 3) (state type:  P ): -5.0\n",
      "Reward for state: (2, 3) (state type:  G ): 1.0\n"
     ]
    }
   ],
   "source": [
    "env_name = \"LavaFloor-v0\"\n",
    "env = gym.make(env_name)\n",
    "print(\"ENVIRONMENT:\\n\")\n",
    "env.render()\n",
    "# states are internally expressed using a numerical index, but you can convert coordinates to indexes:\n",
    "current_state = env.pos_to_state(0, 0) \n",
    "next_state = env.pos_to_state(0, 1)\n",
    "goal_state = env.pos_to_state(2, 3)\n",
    "\n",
    "print(\"\\nNumber of actions: \", env.action_space.n)\n",
    "print(\"Actions: \", env.actions)\n",
    "print(\"Reward of starting state:\", env.RS[current_state])\n",
    "print(\"Reward of goal state:\", env.RS[goal_state])\n",
    "print(\"Probability from (0, 0) to (1, 0) with action left:\", env.T[current_state, 1, next_state])\n",
    "print(\"Probability from (0, 0) to (2, 3) with action left:\", env.T[current_state, 1, goal_state])\n",
    "\n",
    "c=0\n",
    "state_right = env.pos_to_state(0, 1) #state to the tight of start state\n",
    "for i in range(1,101):\n",
    "    current_state = env.pos_to_state(0, 0)\n",
    "    state = env.sample(current_state, 1) #trying to go right\n",
    "    if (state==state_right): \n",
    "        c+=1 #counting how many times the agent reaches the state to the right\n",
    "        \n",
    "#computing percentage of time agent reached the state to the right going right, should be around 80%           \n",
    "print(\"\\nPercentage of time agent reaches the state to the right:\", c/i*100) \n",
    "\n",
    "print(\"\\nTransition model for\", env_name, \":\") #assume transition functions is the same for all states\n",
    "state=0\n",
    "next_state=1\n",
    "for i in range(0,env.action_space.n):\n",
    "    print(\"\\nProbability of reaching\", env.state_to_pos(next_state), \"from\", env.state_to_pos(state), \"executing action\", env.actions[i], \":\", env.T[state, i, next_state])\n",
    "print(\"\\nReward for non terminal states: \",env.RS[env.pos_to_state(0,0)]) #assume all states have same reward\n",
    "for state in range(0,env.observation_space.n):\n",
    "    if env.grid[state] == \"P\" or env.grid[state] == \"G\":\n",
    "                    print(\"Reward for state:\", env.state_to_pos(state) ,\"(state type: \", env.grid[state],\"):\",env.RS[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In addition to the basic LavaFloor environment, you can also test some different environment versions, with different maps and rewards: *BiggerLavaFloor* and *VeryBadLavaFloor*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENVIRONMENT:\n",
      "\n",
      "[['S' 'L' 'L' 'L']\n",
      " ['L' 'W' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n",
      "\n",
      "\n",
      "\n",
      "Executed action U in state (0, 0)\n",
      "Executed action U in state (0, 0)\n",
      "Executed action U in state (0, 0)\n",
      "Executed action U in state (0, 0)\n",
      "Executed action D in state (0, 0)\n",
      "Executed action D in state (1, 0)\n",
      "Executed action D in state (1, 0)\n",
      "Executed action U in state (1, 0)\n",
      "Executed action U in state (0, 0)\n",
      "Executed action L in state (0, 0)\n",
      "Executed action L in state (0, 0)\n",
      "Executed action D in state (1, 0)\n",
      "Executed action D in state (2, 0)\n",
      "Executed action D in state (2, 0)\n",
      "Executed action R in state (2, 0)\n",
      "Executed action R in state (2, 1)\n",
      "Executed action R in state (2, 2)\n",
      "Episode terminated in state  11\n",
      "\n",
      "Goal state reached!\n",
      "\n",
      "--- Evaluation Metrics ---\n",
      "Total Iterations: 850\n",
      "Total Time (seconds): 0.4379\n",
      "New Action Count: 0\n",
      "Tried Action Count: 0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LavaFloor-v0\") # You can also test BiggerLavaFloor-v0 and VeryBadLavaFloor-v0 \n",
    "print(\"ENVIRONMENT:\\n\")\n",
    "env.render()\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "state, metrics = MDP_Lookahead(env, 20, 50) # You might want to change depth and simulation number if testing more complex environments\n",
    "if state == env.goal_state:    \n",
    "    print(\"\\nGoal state reached!\")  \n",
    "else:\n",
    "    print(\"\\nThe agent fell in a pit!\")\n",
    "\n",
    "print(\"\\n--- Evaluation Metrics ---\")    \n",
    "print(f\"Total Iterations: {metrics['total_iterations']}\")\n",
    "print(f\"Total Time (seconds): {metrics['total_time']:.4f}\")\n",
    "print(f\"New Action Count: {metrics['new_action_count']}\")\n",
    "print(f\"Tried Action Count: {metrics['tried_action_count']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
